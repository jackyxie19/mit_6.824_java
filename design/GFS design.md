GFS design

快照

尾部追加

常态化组件故障

大文件

元数据：目录树、访问控制信息、文件到块映射

块租约管理

孤立块垃圾收集：惰性数据清除

块迁移

块复制

块放置策略

块副本检测及恢复

单Master多Server

块大小

操作日志

日志重播恢复状态

松一致性模型

文件命名空间/目录树

检查点

租约变更（Primary、Client、Master、Secondary）

块版本号

叶路径上的完整路径读写锁：锁管理统一由Master负责，Primary只在具体的路径确定后提供读写服务。	

Master与ChunkServer间的注册与监听

shadow：类似mysql的主从概念，master负责写入，shadow响应读取；在Master失效时通过Shadow接替？

# Q&A

1. ChunkServer上是否要存储全量数据的统计信息，Master上已经持有了对文件的引用？
   A：Master设计时为了简便，不在本地缓存文件路径等元数据。而是在每次启动或故障恢复时通过RPC接口从块服务器上拉去文件目录数等信息。Master上的详细目录信息是行ChunkServer上获取的，从最终数据来源以及数据传输便捷的考虑，需要在ChunkServer上维护本机所持有的块信息。
2. ？在单个ChunkServer中的单个file metadata中是否需要保存该块其他副本所在的位置？
   A1：个人认为不需要在每个块上保存其他副本所在的位置，如此保存首先会为系统设计引入复杂性，在每次块副本失效、块复制系数调整时需要更新每个块副本。从功能划分的角度看也不适宜，单个块存储的ChunkServer引入了Master对副本管理的职责，职责划分不是很清晰。对其他副本的感知可在Master加载完所有节点，形成完整的目录树、文件映射表后由Master响应。
3. ？在问题2的基础上，在单个ChunkServer成为Primary后，是否需要感知到其他副本块的位置呢？
   A1：
4. ？如何检测节点故障，以及在检测到故障后如何恢复？
5. ？复制因子是什么层级的设置，全局唯一的设置还是不同副本不同的设置？
6. ？副本的数据检测发生在什么时候，使用什么方式进行数据完整性校验（如校验和）？
   A1：在块复制后进行数据校验。
7. ？在Master失效时通过Shadow接替？
8. ？操作日志如何记录，以访问路径、操作类型决定么，如何关联操作的数据？
   A1：操作日志无需记录读请求，他对数据状态无影响。
9. ？checkpoint包含哪些内容，Master和ChunkServer都需要checkpoint么？若都需要两者的checkpoint有何差异？
10. ？操作日志与checkpoint的边界如何定义，哪些写入checkpoint，哪些写入操作日志？
11. ？操作日志和checkpoint的备份如何复制到不同的节点，如何选择两者复制到的节点？
12. ？在进行故障恢复时，多副本的操作日志和checkpoint以哪个副本为准进行恢复呢？
    A1：对每个操作日志及checkpoint增加类似版本号的概念，保证至少不会以老旧的副本回滚。
13. ？操作日志记录与记录提交之间的关系？在哪个时间点将变更视为提交？
14. ？读取文件时需要比对校验和，块本身存储完成后有一个校验和，另一个用于比对的校验和是从哪里来的？是否Master的目录树或者路径文件映射中需要再存放一份校验和？




gfs

# 0、摘要

​	Google 文件系统（GFS）是一种由 Google 设计和实现的文件系统。它是一种可扩展的分布式文件系统，专为大型分布式数据密集型应用程序而设计。其提供容错功能，即使出现硬件故障或其他问题，可以正常运行。设计时考虑运行在廉价机器上，因此比其他类型的文件系统成本更低。GFS可以同时处理大量数据和请求。虽然 Google 文件系统的许多目标与以前的分布式文件系统相同，但其设计是由对 Google 应用程序负载和技术环境的需要推动的。GFS的设计与早期文件系统假设背道而驰，这促使人们对截然不同的设计要点进行了探索。GFS成功满足谷歌的存储需求，并在公司内部广泛部署，用于生成和处理其服务所使用的数据存储平台。GFS中最大的集群有一千多台计算机上、数千个磁盘上，提供数百 TB 的存储空间，并且可以由数百个客户端同时访问。本文介绍了旨在支持分布式应用程序的文件系统接口扩展，讨论了谷歌文件系统设计的许多方面，并报告了微基准测试和实际使用的测量结果。



# 1、介绍

​	Google 文件系统 (GFS) 的设计和实施旨在满足谷歌不断增长的数据处理需求。GFS 与以前的分布式文件系统有着相似的目标，例如性能、可扩展性、可靠性和可用性。但是，GFS的设计是由对Google应用负载和技术环境的需求推动的，做出了部分与早期文件系统设计假设不同的地方。

​	**常态化的组件故障**：组件故障在 GFS 中很常见，因为文件系统由数百甚至数千台存储计算机组成，这些存储计算机由廉价的商品部件构建，并且可供相当数量的客户端计算机访问。组件的数量和质量决定了某些组件在任何给定时间都无法正常工作，有些组件无法从当前的故障中恢复过来。已观察到由应用程序错误、操作系统错误、人为错误以及磁盘、内存、连接器、网络和电源故障引起的问题。通过**持续监控**、**错误检测**、**容错**和**自动恢复**以保障系统可靠性和可用性。

​	**适配大文件**：GFS 处理大小为多 GB 的大文件，与处理较小文件的传统文件系统不同。在处理包含数十亿个对象、 TB级 快速增长的数据集时，传统文件系统需要管理数十亿个大约 KB 大小的文件，显得很笨拙。由于文件很大，必须重新设计文件系统的，例如 I/O 操作和块大小，以优化性能。

​	**尾部追加**：GFS 中的大多数文件都是通过追加数据写入的，不存在随机修改。一旦写入，文件只能被读取，而且通常只能按顺序读取。鉴于这种对大文件的访问模式，追加成为性能优化和原子性保证的重点，而在客户端中缓存数据块则失去了吸引力。
​	**统一设计应用程序和文件系统 API** 以提高灵活性，从而使整个系统受益。例如，GFS 放宽一致性模型，以简化文件系统。还引入了原子追加操作，这样多个客户端就可以同时追加到一个文件中，而无需进行额外的同步。本文稍后将详细讨论这些设计选择。
目前部署了GFS集群中，最大的拥有超过 1000 个存储节点，超过 300 TB 的磁盘存储，并且由不同计算机上的数百个客户端连续大量访问。



# 2、设计概览

​	GFS适用于大型分布式数据密集型应用程序。 该设计由对需求驱动，与之前的文件系统不同。 该系统在廉价的商用硬件上运行时提供容错能力，并为大量客户机提供高聚合性能。 



## 2.1、假设

-   GFS 由容易出现故障的**廉价商用机**构建。因此，系统必须持续监控自身，并定期检测、容忍组件故障，并迅速从组件故障中恢复。
-   GFS 旨在存储**少量的大文件**，预计有几百万个文件，每个文件的大小通常为 100 MB 或更大。多 GB 文件是常见情况，应加以有效管理。但是，必须支持小文件，但系统无需针对它们进行优化。
-   工作负载主要由两种读取组成：**大型流式读取**和**小型随机读取**。在大型流式读取中，单个操作通常读取数百 KB，更常见的是 1 MB 或更多。来自同一个客户端的连续操作通常会读取文件的连续区域。一个小的随机读取通常在某个任意偏移量下读取几KB。注重性能的应用程序通常会对其小读取进行批处理和排序，以稳定地浏览文件，而不是来回移动。
-   工作负载还有许多**大规模的顺序写入**，这些写入操作会将数据附加到文件中。写操作大小与读操作大小相似。写入文件后，很少会再次修改文件。支持在文件中任意位置进行小写入操作，但不一定要高效。
-   GFS 需要同时处理多个客户端向一个文件追加的请求。这些文件通常用作生产者-消费者队列或用于多向合并。数百个生产者（每台计算机运行一个）将同时追加到一个文件中。 最小的同步开销意味着系统应尽可能少地使用同步来实现原子性，因为同步可能是分布式系统的性能瓶颈。系统还必须确保稍后可以读取该文件，可以由其他客户端读取 。
-   GFS 针对于数据密集型应用，需要处理大量数据，高带宽比低延迟更重要。系统需要能够快速高效地处理大量数据，而不是专注于最大限度地减少传输单个数据所花费的时间。 这与传统文件系统的假设背道而驰，后者优先考虑低延迟而不是高带宽。 通过优先考虑高带宽，GFS能够满足其目标应用程序的需求，并为大量客户提供高综合性能。



## 2.2、接口

​	GFS 提供了用户熟悉的文件系统接口，但是没有实现诸如POSIX之类的标准API，这是一组操作系统标准。GFS 中的文件按目录层次结构组织，并由路径名标识，这是在文件系统中组织文件的常用方法。GFS 支持创建、删除、打开、关闭、读取和写入文件的常规操作，这些是文件系统的基本文件操作。
​	GFS 还具有快照和记录追加操作，这是传统文件系统中都没有的独特功能。Snapshot 以低成本创建文件或目录树的副本，这对于在不影响原始文件的情况下创建备份或测试更改非常有用。Record append 允许多个客户端同时向同一个文件追加数据，同时保证每个客户端追加的原子性。这对于实现多路合并结果和生产者-消费者队列很有用，许多客户端可以同时追加到这些队列而无需加锁。这些类型的文件对于构建大型分布式应用程序非常有用，这是 GFS 的主要用途。快照和记录附录分别在第 3.4 节和第 3.3 节中进行了进一步讨论，这两节提供了有关这些功能的更多详细信息。



## 2.3、架构

​	GFS由单个主服务器和多个区块服务器组成，可被多个客户端访问。只要计算机资源允许，可在同一台计算机上同时运行区块服务器和客户端，并且接收较低的可靠性。
​	GFS 中的文件分为固定大小的块，每个区块由创建区块时主服务器分配的不可变且全局唯一的 64 位区块句柄标识。Chunkservers 将区块作为 Linux 文件存储在本地磁盘上，并读取或写入由区块句柄和字节范围指定的区块数据。为了提高可靠性，每个区块都在多个区块服务器上备份。默认情况下，GFS 存储三个副本，但用户可以为文件命名空间的不同区域指定不同的复制级别。在多个区块服务器上复制区块可确保在出现硬件故障或其他问题时数据不会丢失。使用固定大小的区块可实现高效的数据处理，并减少网络延迟对性能的影响。

​	Master 负责维护所有文件系统**元数据**，包括各种类型的信息，例如命名空间（目录树）、访问控制信息、从文件到块的映射以及块的当前位置。Master 还控制系统范围内的活动，例如**块租约管理**，其中包括将租约分配给 chunkserver，以确保它们在一定时期内对特定区块具有独占访问权限，有助于防止冲突并确保数据一致性。孤立块的**垃圾收集**是 Master 执行的另一项重要任务，它涉及识别和删除不再需要或已损坏的块。Master 也负责管理块服务器之间的**块迁移**，包括将块从一台服务器移动到另一台服务器以平衡负载并确保最佳性能。Master 与每个块服务器维持**心跳**，以发送执行并收集状态信息。这使主机能够监控系统的运行状况并根据需要进行调整，以确保其继续平稳运行。

​	GFS 客户端代码链接到每个应用程序，该应用程序实现文件系统 API，并与 Master 和 Chunkservers 通信，代表应用程序读取或写入数据。 Client 与 Master 间交互元数据变更，但真实数据都直接发送到 ChunkServers。 GFS 不提供 POSIX API，因此不需要挂接 Linux vnode 层。

​	客户端和块服务器都不会缓存文件数据。客户端缓存几乎没有什么好处，因为大多数应用程序都会流式传输大文件或工作集太大而无法缓存。 没有客户端缓存可以消除缓存一致性问题，从而简化客户端和整个系统。但客户端会缓存元数据。 ChunkServers 不需要缓存文件数据，因为块存储为本地文件，而且 Linux 的缓冲区缓存已经将经常访问的数据保存在内存中。

​	

## 2.4、单主架构

​	GFS 只有一个 Master，它简化了设计，使 Master 能够利用全局信息做出复杂的块放置和块复制决策。 单主架构可以更好地协调和管理分布式文件系统。 Master 负责维护文件命名空间并跟踪每个数据块的位置。还负责将区块分配给区块服务器，并确保每个区块都有足够的副本以实现容错。 但是，必须尽量减少主节点参与读取和写入的情况，以防止其成为瓶颈。 客户端从不通过主节点读取和写入文件数据，以避免请求过载。 取而代之的是，客户端会询问主节点他们应该联系哪些区块服务器来读取或写入数据。 客户端会限时缓存元数据，并直接与块服务器交互。 此架构允许高效、可扩展的数据访问，同时仍能保持容错和一致性。

![image-20230812230839569](./images/gfs/image-20230812230839569.png)

​	通过图1来介绍简单的读流程。第一步是让客户端通过固定的块大小将应用指定的文件名和字节偏移量转换为文件内的块索引。 确定区块索引后，客户端会向 Master 发送包含文件名和块索引的请求。 然后，Master 用将块句柄和副本的位置返回客户端。 客户端使用文件名和块索引作为密钥缓存此信息，避免再与 Master 进行不必要的交互。缓存信息后，客户端向其中一个副本（通常是最接近的副本）发送请求，指定块句柄和该区块内的字节范围。 如果客户端需要再次读取相同的区块，在缓存的信息过期或文件重新打开前无需与 Master 交互。 实际上，客户端可以在同一个请求中请求多个块，而 Master 也同时返回多个块索引，这有助于减少后续客户端与 Master 的交互，而无额外开销。



## 2.5、块大小

​	块大小是GFS的核心参数。块大小设为 64 MB，比传统文件系统大得多。每个块副本都作为普通 Linux 文件存储在 chunkserver 上，并且仅在需要时进行扩展，这样可以避免因内部碎片而浪费空间。

​	大块设置有很多优势，例如减少 Client 与 Master 的交互，因为对同一个块进行读写只需要向主节点发出一次初始请求即可获取块位置信息。减少与 Master 的交互可大幅降低GFS的工作负载，因为应用程序大多是顺序读取和写入大文件。即使是小规模的随机读取，客户端也可以轻松地缓存多 TB数据集的所有块位置信息。在较大的块上，客户端很可能需要对单个块进行多次访问，可通过保持与块服务器的持续 TCP 连接来减少网络开销。较大的块大小还会减小存储在主服务器上的**元数据**的大小，从而使 GFS 元数据可存内存中而不担心内存空间，将在第 2.6.1 节中讨论。

​	即使使用延迟的空间分配，大块大小也有其缺点。一个小文件由少量块组成，可能只有一个块，如果许多客户端都在访问同一个文件，则存储这些块的块服务器可能会成为热点。实际上，热点并不是主要问题，因为应用主要按顺序读取大型多块文件。

​	当批处理队列系统首次使用 GFS 时，确实出现了热点，在这种系统中，可执行文件作为单块文件写入 GFS，然后同时在数百台计算机上启动。存储此可执行文件的几个区块服务器因数百个同步请求而过载，但是 Google 通过提高此类可执行文件的副本数量，以及打散应用的启动时间来解决此问题。
解决热点问题的可行长期解决方案是允许客户端在这种情况下从其他客户端读取数据（类似peer-to-peer）。



## 2.6、元数据

​	GFS 的 Master 存储三种类型的元数据。第一种元数据是文件和块命名空间（目录树），用于跟踪系统中的文件和区块。第二种是从文件到块的映射，用于定位构成文件的块。所有元数据都保存在 Master 中，便于快速访问和检索信息。为保障元数据的持久性，系统会在 Master 上将变更落盘并在复制到远程的操作日志中。日志提供了对 Master 状态简单可靠的更新，在 Master 宕机时不用担心一致性问题。但是，Master 不会永久存储块位置信息。在 Master 启动以及有新的块服务器加入集群时，向每个块服务器拉去块信息。这种方法允许系统动态调整群集中的变化，例如添加或删除区块服务器。总体而言，Master 在管理 GFS 元数据方面起着至关重要的作用，其设计是性能、可靠性和可扩展性之间的平衡。



### 2.6.1、内存数据结构

​	GFS 将元数据存储在内存中，Master 可以快速的执行命令。由于元数据都在内存中，Master 可以在快速高效地定期扫描全部状态。这种周期任务用于各种活动，例如区块垃圾收集、块服务器故障时的重新复制以及块迁移以平衡块服务器之间的负载和磁盘空间使用情况。4.3、4.4节会详细介绍这些内容。

​	纯内存方式的一个潜在问题是，块的数量以及整个系统的容量都受到 Master 内存容量的限制。但是，这在实践中并不是一个严重的限制，因为 Master 为每个 64 MB 区块保留的元数据少于 64 字节。由于顺序填充，文件的大多数块是填满的，只有最后一个可能被部分填满，所以单个大文件对应的块数量不多。通常每个文件命名空间所需的字节少于 64 字节，因为它使用前缀压缩来紧凑地存储文件名。

​	如果需要支持更大容量的文件系统，向 Master 添加额外内存的成本很小，而将元数据存储在内存中可获得的简单性、可靠性、性能和灵活性。



### 2.6.2、块地址

>   Master 控制着块信息的变更，无需块服务器定时上报块变更？

​	Master 不会持久化块服务器上的块信息。相反，它会在启动时轮询 chunkservers 以获取这些信息。 此后，主节点可以保持最新状态，因为它可以控制所有区块的位置，并通过常规的 HeartBeat 消息监视块服务器状态。 

​	最初，作者试图将块位置信息永久保存在 Master，但在启动时以及之后定期向块服务器请求数据要简单得多。 此设计消除了在 chunkservers 加入和离开集群、更改名称、失败、重启等时保持 Master 和块服务器同步的问题。在拥有数百台服务器的集群中，这些事件经常发生。

​	此外，块服务器在自己的磁盘上有哪些区块或没有哪些区块的最终决定权在于 chunkserver 本身。因此，在 Master 上保持对这些信息的一致视图是没有意义的，因为块服务器上的错误可能会导致块自发消失（例如，磁盘可能变坏并被禁用），或者人为重命名 chunkserver。如果 Master 要维持每个块服务器上块信息的一致视图，则需要不断检查和更新这些信息，这会效率低下而且可能不可靠。通过允许每个 chunkserver 管理自己的区块，系统可以提高容错能力和对错误的弹性。



### 2.6.3、操作日志

​	操作日志是 GFS 的重要组成部分，其中包含关键元数据变更的历史记录。它是元数据的唯一持久化记录，也是定义并发操作顺序的逻辑时间表。创建文件、区块及其版本的逻辑时间只由操作日志识别。

​	由于操作日志至关重要，因此必须对其进行可靠存储，并在变更持久化前，不应让客户端感知变更。如果在变更持久化前已对客户端可见，则即使块本身仍然存在，整个文件系统或最近的客户端操作也可能会丢失。为确保可靠性，将在多节点上复制操作日志，只有在本地和远程都将日志落盘后，才会响应客户端操作。Master 在刷新前将多个日志打包批处理，从而减少刷新和复制对整体系统吞吐量的影响。

​	Master 通过重播操作日志来恢复其文件系统状态。 为了最大限度地缩短启动时间，日志必须保持较小。 每当日志超过一定大小时，Master 都会检查其状态。 检查点以类似 B 树的紧凑形式创建，可以直接映射到内存中，无需额外解析即可用于命名空间查找。 这进一步加快了恢复速度并提高了可用性。

​	由于构建检查点可能需要一段时间，Master 内部状态的设计成可以在创建新的检查点时不阻塞变更。 Master 切换到新的日志文件并在单独的线程中创建新的检查点（copy 某一瞬间的快照？）。新的检查点包括切换前的所有变更，对于包含几百万个文件的集群，可以在一分钟左右的时间内创建。 完成后，新的检查点将在本地和远程写入磁盘。

​	恢复只需要最新的完整检查点和后续的日志文件。 可以自由删除较旧的检查点和日志文件，但为了防范灾难，保留了一些检查点和日志文件。 检查点操作期间出现故障不会影响正确性，因为恢复代码会检测并跳过不完整的检查点。



## 2.7 一致性模型

​	GFS 采用松一致性模型，以支持高度分布的应用，系统的设计易于使用和维护，在一致性和性能之间进行的权衡。 尽管其一致性模型比较宽松，但 GFS 仍然为应用程序提供数据准确性和时效性保障。GFS如何维持这些担保的细节留给了本文的其他部分。  

​	

### 2.7.1、GFS提供的保障

​	文件命名空间的变更（例如文件创建）是原子的，这些变更被视为单一的、不可分割的操作，它们要么全完成，要么全不完成。 在 GFS 中，文件命名空间变更仅由 Master 处理，Master 负责维护文件系统的整体状态。 Master 使用命名空间锁确保原子性和正确，以防止多个客户端同时修改同一个文件命名空间。 此外，Master 的操作日志定义了这些操作的全局总顺序，这有助于确保所有 Client 在任何给定时间都能看到相同的系统状态。

![image-20230813135224679](./images/gfs/image-20230813135224679.png)

​	数据变更后文件区域的状态取决于多种因素，包括变更的类型、成功还是失败以及是否存在并发修改。 表 1 总结了这些变更的可能结果，包括受影响区域是已定义（即一致）还是未定义（即不一致）。 定义的区域是指所有客户端将始终看到相同的数据，无论他们从哪个副本读取数据，并且如果文件数据变更一致，并且客户端将看到变更写入的全部内容，则该区域是在文件数据变更之后定义的。 并发成功的修改会使该区域未定义但保持一致，这意味着所有客户端都能看到相同的数据，但可能无法反映任何一个变更所写的内容。 变更失败会使该区域不一致且未定义，这意味着不同的 Client 可能会在不同的时间看到不同的数据。 使用 GFS 的应用可以区分已定义区域和未定义区域，但它们无需区分不同类型的未定义区域。

​	GFS有两种数据变更类型：写入和记录追加。写入操作向指定的文件偏移量写入数据。记录追加操作至少会以原子方式追加一次数据，即使存在并发修改也是如此，但偏移量由 GFS 选择。相比之下，常规的追加操作只是 Client 感知到的是在当前文件的末尾执行写入操作。返回给客户端的偏移量指向包含该记录的已定义区域的开始。GFS 可能会在定义的区域之间插入填充或重复记录项，这些区域被认为不一致，通常比用户数据小得多。

​	经过一系列成功的变更后，可以保证变更的文件区域已定义，并包含最后一次变更写入的数据。GFS 通过在所有副本上以相同顺序对区块应用变更来实现这一目标，并使用块版本号来检测任何因在 chunkserver 异常未变更的过时副本。旧的副本永远不会参与更新，位置信息也不会被 Master 返回给 Client，其会尽早被垃圾收集。

​	Client 可以在信息刷新前从旧的副本中读取数据，在缓存失效或再次打开文件时会更新副本元数据
由于 GFS 中的大多数文件都是仅追加的，因此旧副本通常会返回过早的区块结尾，而不是过时的数据。
当读者重试并联系主服务器时，它会立即获得当前的区块位置。GFS 通过主服务器和所有区块服务器之间的心跳来识别异常块服务器，并通过校验和检测数据损坏。发现问题后，数据会尽快从有效副本中恢复，只有在 GFS 做出反应之前（通常在几分钟之内）所有副本都丢失时，区块才会不可逆转地丢失。
即使在这种情况下，区块只会变得不可用而不是损坏，应用程序会收到明显的错误而不是损坏的数据。



### 2.7.2、应用实现

​	为了实现松一致性模型，GFS 使用了一些已经用于其他目的的简单技术。有记录追加、检查点和写入自校验功能的记录。

​	其中一种技术是在变更文件时依赖于**追加而不是覆盖**。这意味着不是覆盖现有数据，而是将新数据添加到文件末尾。 这种方法比随机写入更高效，更能抵御应用程序故障，因为随机写入可能会导致数据损坏或丢失。 

​	GFS 使用的另一种技术是**检查点**，它定期保存文件状态，以确保在出现故障时数据不会丢失。 检查点还可能包括应用程序级校验和，以确保数据的完整性。 文件读取器仅处理直到最后一个检查点的数据，该检查点已知处于已定义状态。

​	第三种技术是写入**自我验证、自我识别**的记录。 如果多个写入器同时追加到一个文件中，GFS 会使用至少追加一次的语义来保留每个写入器的输出。此操作可能会导致偶尔出现填充和重复，读者需要处理这些问题。 每条记录都包含额外的信息，例如校验和以验证其有效性。 可以使用校验和识别和丢弃多余的填充和记录片段，并使用记录中的唯一标识符过滤掉重复项。这些用于记录 I/O 的功能位于 GFS 应用程序共享的库代码中，可应用于 Google 的其他文件接口实现。

​	总体而言，这些技术允许 GFS 为大量客户端提供容错和高性能，同时适应宽松的一致性模型。



# 3、系统交互

​	GFS的设计，旨在最大限度地减少Master的负载，避免 Master 成为大规模分布式系统的瓶颈或单点故障。 文本继续描述 Client、Master 和 ChunkServer 间的交互，如何实现数据变更、记录的原子追加和快照。



## 3.1、 租约和变更顺序

​	GFS 中，变更是指更改块内容或元数据的操作，例如写入或追加操作。每次变更都是在块的所有副本上执行的，以确保容错和数据一致性。系统使用租约保证副本之间一致的变更顺序。Master 向其中一个副本（称为主副本 primary）授予区块租约。然后，主副本为块的所有变更指定一个顺序，在更新时所有副本都遵循此顺序。因此，全局变更顺序由 Master 选择的租约授予顺序定义，在租约中由主副本分配的序列号定义。
​	租约旨在最大限度地减少 Master 的管理开销，租约本质上是 chunkserver 对数据块执行变更的限时许可。租约的初始超时时间为 60 秒，但只要区块发生变化，主副本就可以无限期地向 Master 请求延期。
延期请求和授权随着 Master 和所有 chunkserver 间的 HeartBeat 传递。有时，Master 可能会尝试在租约到期之前撤销租约，例如当 Master 想要对正在重命名的文件禁用变更时。即使 Master 与主副本间断通信终端，它也可以在旧租约到期后安全地向另一个副本授予新的租约。

![image-20230813220055493](./images/gfs/image-20230813220055493.png)

​	如图 2 中描述了写入请求的控制流和数据流。 

>   步骤3中是client向所有的chunkserver推送数据，而不是client将数据推送道primary，再由primary实现多副本冗余。减少chunkserver间多余的io消耗，也简化chunkserver的代码实现。

1.   Client 向 Master 询问是哪个 chunkserver 持有当前块的租约，以及其他副本的位置。如果没有分配租约，Master 会选择一个副本授予其租约（图中未展示）。 
2.   主副本（Primary）响应 Client，消息中包含其主副本的身份认证和其他副本的位置。Client 缓存这些数据用于后续变更，只有在主副本无法访问或响应其不再持有租约时才需要再次联系 Master。
3.   Client 按将数据推送到所有副本，不保证副本间的先后顺序。每个 chunkserver 将数据缓存在内部 LRU 缓冲区中，直到数据被使用或过时。 通过将数据流与控制流分离，无论哪个块服务器是主副本，系统都可以根据网络拓扑调度昂贵的数据流，从而提高性能，3.2节会详细介绍。
4.   一旦所有副本都确认接收了数据，Client 就会向 Primary 发送写入请求，请求可标识上一步写入数据。主副本向主副本分配连续的序列号，可能来自多个客户端，从而提供必要的序列化。 Primary 为收到的变更分配连续的序列号，这些变更可能来自多个 Client。Client按照序列号顺序将变更应用于自己的本地状态。
5.   Primary 将写入请求转发给所有Secondary 副本。 每个辅助副本都按照主副本分配的相同序列号顺序应用变更。
6.   在 Secondary 完成变更后会 ack 主副本。 
7.   Primary 回复 Client，任何副本遇到的任何错误都会报告给客户端。如果出现错误，Primary 和 Secondary 可能存在部分写入成功部分写入失败。（如果故障在 Primary，则其不会生成序列号并执行后续的下发操作）。Client 请求会被视为失败，变更后的区域也处于不一致状态。 在完全退回到写入最开始前，Client 会尝试重试 (3) 到 (7) 步来处理此类错误。

​	如果写入较大或跨多个块（在 GFS 中，区块是固定大小的数据单位），Client 会将其分解为多个较小的写入操作。 这些较小的写入操作与常规写入操作流程相同，但它们可能会与其他 Client 的并发操作交错而被覆盖。 尽管副本（存储在不同计算机上的文件副本）是一致的，因为所有副本上的各个操作都以相同的顺序成功完成，但共享文件区域最终可能包含来自不同 Client 的数据。 这使得该区域将处于一致但未定义的状态，如本文第 2.7 节所述。 



## 3.2 数据流


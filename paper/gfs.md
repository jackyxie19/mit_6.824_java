# gfs

# 0、摘要

​	Google 文件系统（GFS）是一种由 Google 设计和实现的文件系统。它是一种可扩展的分布式文件系统，专为大型分布式数据密集型应用程序而设计。其提供容错功能，即使出现硬件故障或其他问题，可以正常运行。设计时考虑运行在廉价机器上，因此比其他类型的文件系统成本更低。GFS可以同时处理大量数据和请求。虽然 Google 文件系统的许多目标与以前的分布式文件系统相同，但其设计是由对 Google 应用程序负载和技术环境的需要推动的。GFS的设计与早期文件系统假设背道而驰，这促使人们对截然不同的设计要点进行了探索。GFS成功满足谷歌的存储需求，并在公司内部广泛部署，用于生成和处理其服务所使用的数据存储平台。GFS中最大的集群有一千多台计算机上、数千个磁盘上，提供数百 TB 的存储空间，并且可以由数百个客户端同时访问。本文介绍了旨在支持分布式应用程序的文件系统接口扩展，讨论了谷歌文件系统设计的许多方面，并报告了微基准测试和实际使用的测量结果。



# 1、介绍

​	Google 文件系统 (GFS) 的设计和实施旨在满足谷歌不断增长的数据处理需求。GFS 与以前的分布式文件系统有着相似的目标，例如性能、可扩展性、可靠性和可用性。但是，GFS的设计是由对Google应用负载和技术环境的需求推动的，做出了部分与早期文件系统设计假设不同的地方。

​	**常态化的组件故障**：组件故障在 GFS 中很常见，因为文件系统由数百甚至数千台存储计算机组成，这些存储计算机由廉价的商品部件构建，并且可供相当数量的客户端计算机访问。组件的数量和质量决定了某些组件在任何给定时间都无法正常工作，有些组件无法从当前的故障中恢复过来。已观察到由应用程序错误、操作系统错误、人为错误以及磁盘、内存、连接器、网络和电源故障引起的问题。通过**持续监控**、**错误检测**、**容错**和**自动恢复**以保障系统可靠性和可用性。

​	**适配大文件**：GFS 处理大小为多 GB 的大文件，与处理较小文件的传统文件系统不同。在处理包含数十亿个对象、 TB级 快速增长的数据集时，传统文件系统需要管理数十亿个大约 KB 大小的文件，显得很笨拙。由于文件很大，必须重新设计文件系统的，例如 I/O 操作和块大小，以优化性能。

​	**尾部追加**：GFS 中的大多数文件都是通过追加数据写入的，不存在随机修改。一旦写入，文件只能被读取，而且通常只能按顺序读取。鉴于这种对大文件的访问模式，追加成为性能优化和原子性保证的重点，而在客户端中缓存数据块则失去了吸引力。
​	**统一设计应用程序和文件系统 API** 以提高灵活性，从而使整个系统受益。例如，GFS 放宽一致性模型，以简化文件系统。还引入了原子追加操作，这样多个客户端就可以同时追加到一个文件中，而无需进行额外的同步。本文稍后将详细讨论这些设计选择。
目前部署了GFS集群中，最大的拥有超过 1000 个存储节点，超过 300 TB 的磁盘存储，并且由不同计算机上的数百个客户端连续大量访问。



# 2、设计概览

​	GFS适用于大型分布式数据密集型应用程序。 该设计由对需求驱动，与之前的文件系统不同。 该系统在廉价的商用硬件上运行时提供容错能力，并为大量客户机提供高聚合性能。 



## 2.1、假设

-   GFS 由容易出现故障的**廉价商用机**构建。因此，系统必须持续监控自身，并定期检测、容忍组件故障，并迅速从组件故障中恢复。
-   GFS 旨在存储**少量的大文件**，预计有几百万个文件，每个文件的大小通常为 100 MB 或更大。多 GB 文件是常见情况，应加以有效管理。但是，必须支持小文件，但系统无需针对它们进行优化。
-   工作负载主要由两种读取组成：**大型流式读取**和**小型随机读取**。在大型流式读取中，单个操作通常读取数百 KB，更常见的是 1 MB 或更多。来自同一个客户端的连续操作通常会读取文件的连续区域。一个小的随机读取通常在某个任意偏移量下读取几KB。注重性能的应用程序通常会对其小读取进行批处理和排序，以稳定地浏览文件，而不是来回移动。
-   工作负载还有许多**大规模的顺序写入**，这些写入操作会将数据附加到文件中。写操作大小与读操作大小相似。写入文件后，很少会再次修改文件。支持在文件中任意位置进行小写入操作，但不一定要高效。
-   GFS 需要同时处理多个客户端向一个文件追加的请求。这些文件通常用作生产者-消费者队列或用于多向合并。数百个生产者（每台计算机运行一个）将同时追加到一个文件中。 最小的同步开销意味着系统应尽可能少地使用同步来实现原子性，因为同步可能是分布式系统的性能瓶颈。系统还必须确保稍后可以读取该文件，可以由其他客户端读取 。
-   GFS 针对于数据密集型应用，需要处理大量数据，高带宽比低延迟更重要。系统需要能够快速高效地处理大量数据，而不是专注于最大限度地减少传输单个数据所花费的时间。 这与传统文件系统的假设背道而驰，后者优先考虑低延迟而不是高带宽。 通过优先考虑高带宽，GFS能够满足其目标应用程序的需求，并为大量客户提供高综合性能。



## 2.2、接口

​	GFS 提供了用户熟悉的文件系统接口，但是没有实现诸如POSIX之类的标准API，这是一组操作系统标准。GFS 中的文件按目录层次结构组织，并由路径名标识，这是在文件系统中组织文件的常用方法。GFS 支持创建、删除、打开、关闭、读取和写入文件的常规操作，这些是文件系统的基本文件操作。
​	GFS 还具有快照和记录追加操作，这是传统文件系统中都没有的独特功能。Snapshot 以低成本创建文件或目录树的副本，这对于在不影响原始文件的情况下创建备份或测试更改非常有用。Record append 允许多个客户端同时向同一个文件追加数据，同时保证每个客户端追加的原子性。这对于实现多路合并结果和生产者-消费者队列很有用，许多客户端可以同时追加到这些队列而无需加锁。这些类型的文件对于构建大型分布式应用程序非常有用，这是 GFS 的主要用途。快照和记录附录分别在第 3.4 节和第 3.3 节中进行了进一步讨论，这两节提供了有关这些功能的更多详细信息。



## 2.3、架构

​	GFS由单个主服务器和多个区块服务器组成，可被多个客户端访问。只要计算机资源允许，可在同一台计算机上同时运行区块服务器和客户端，并且接收较低的可靠性。
​	GFS 中的文件分为固定大小的块，每个区块由创建区块时主服务器分配的不可变且全局唯一的 64 位区块句柄标识。Chunkservers 将区块作为 Linux 文件存储在本地磁盘上，并读取或写入由区块句柄和字节范围指定的区块数据。为了提高可靠性，每个区块都在多个区块服务器上备份。默认情况下，GFS 存储三个副本，但用户可以为文件命名空间的不同区域指定不同的复制级别。在多个区块服务器上复制区块可确保在出现硬件故障或其他问题时数据不会丢失。使用固定大小的区块可实现高效的数据处理，并减少网络延迟对性能的影响。

​	Master 负责维护所有文件系统**元数据**，包括各种类型的信息，例如命名空间（目录树）、访问控制信息、从文件到块的映射以及块的当前位置。Master 还控制系统范围内的活动，例如**块租约管理**，其中包括将租约分配给 chunkserver，以确保它们在一定时期内对特定区块具有独占访问权限，有助于防止冲突并确保数据一致性。孤立块的**垃圾收集**是 Master 执行的另一项重要任务，它涉及识别和删除不再需要或已损坏的块。Master 也负责管理块服务器之间的**块迁移**，包括将块从一台服务器移动到另一台服务器以平衡负载并确保最佳性能。Master 与每个块服务器维持**心跳**，以发送执行并收集状态信息。这使主机能够监控系统的运行状况并根据需要进行调整，以确保其继续平稳运行。

​	GFS 客户端代码链接到每个应用程序，该应用程序实现文件系统 API，并与 Master 和 Chunkservers 通信，代表应用程序读取或写入数据。 Client 与 Master 间交互元数据变更，但真实数据都直接发送到 ChunkServers。 GFS 不提供 POSIX API，因此不需要挂接 Linux vnode 层。

​	客户端和块服务器都不会缓存文件数据。客户端缓存几乎没有什么好处，因为大多数应用程序都会流式传输大文件或工作集太大而无法缓存。 没有客户端缓存可以消除缓存一致性问题，从而简化客户端和整个系统。但客户端会缓存元数据。 ChunkServers 不需要缓存文件数据，因为块存储为本地文件，而且 Linux 的缓冲区缓存已经将经常访问的数据保存在内存中。

​	

## 2.4、单主架构

​	GFS 只有一个 Master，它简化了设计，使 Master 能够利用全局信息做出复杂的块放置和块复制决策。 单主架构可以更好地协调和管理分布式文件系统。 Master 负责维护文件命名空间并跟踪每个数据块的位置。还负责将区块分配给区块服务器，并确保每个区块都有足够的副本以实现容错。 但是，必须尽量减少主节点参与读取和写入的情况，以防止其成为瓶颈。 客户端从不通过主节点读取和写入文件数据，以避免请求过载。 取而代之的是，客户端会询问主节点他们应该联系哪些区块服务器来读取或写入数据。 客户端会限时缓存元数据，并直接与块服务器交互。 此架构允许高效、可扩展的数据访问，同时仍能保持容错和一致性。

![image-20230812230839569](./images/gfs/image-20230812230839569.png)

​	通过图1来介绍简单的读流程。第一步是让客户端通过固定的块大小将应用指定的文件名和字节偏移量转换为文件内的块索引。 确定区块索引后，客户端会向 Master 发送包含文件名和块索引的请求。 然后，Master 用将块句柄和副本的位置返回客户端。 客户端使用文件名和块索引作为密钥缓存此信息，避免再与 Master 进行不必要的交互。缓存信息后，客户端向其中一个副本（通常是最接近的副本）发送请求，指定块句柄和该区块内的字节范围。 如果客户端需要再次读取相同的区块，在缓存的信息过期或文件重新打开前无需与 Master 交互。 实际上，客户端可以在同一个请求中请求多个块，而 Master 也同时返回多个块索引，这有助于减少后续客户端与 Master 的交互，而无额外开销。



## 2.5、块大小

​	块大小是GFS的核心参数。块大小设为 64 MB，比传统文件系统大得多。每个块副本都作为普通 Linux 文件存储在 chunkserver 上，并且仅在需要时进行扩展，这样可以避免因内部碎片而浪费空间。

​	大块设置有很多优势，例如减少 Client 与 Master 的交互，因为对同一个块进行读写只需要向主节点发出一次初始请求即可获取块位置信息。减少与 Master 的交互可大幅降低GFS的工作负载，因为应用程序大多是顺序读取和写入大文件。即使是小规模的随机读取，客户端也可以轻松地缓存多 TB数据集的所有块位置信息。在较大的块上，客户端很可能需要对单个块进行多次访问，可通过保持与块服务器的持续 TCP 连接来减少网络开销。较大的块大小还会减小存储在主服务器上的**元数据**的大小，从而使 GFS 元数据可存内存中而不担心内存空间，将在第 2.6.1 节中讨论。

​	即使使用延迟的空间分配，大块大小也有其缺点。一个小文件由少量块组成，可能只有一个块，如果许多客户端都在访问同一个文件，则存储这些块的块服务器可能会成为热点。实际上，热点并不是主要问题，因为应用主要按顺序读取大型多块文件。

​	当批处理队列系统首次使用 GFS 时，确实出现了热点，在这种系统中，可执行文件作为单块文件写入 GFS，然后同时在数百台计算机上启动。存储此可执行文件的几个区块服务器因数百个同步请求而过载，但是 Google 通过提高此类可执行文件的副本数量，以及打散应用的启动时间来解决此问题。
解决热点问题的可行长期解决方案是允许客户端在这种情况下从其他客户端读取数据（类似peer-to-peer）。



## 2.6、元数据

​	GFS 的 Master 存储三种类型的元数据。第一种元数据是文件和块命名空间（目录树），用于跟踪系统中的文件和区块。第二种是从文件到块的映射，用于定位构成文件的块。所有元数据都保存在 Master 中，便于快速访问和检索信息。为保障元数据的持久性，系统会在 Master 上将变更落盘并在复制到远程的操作日志中。日志提供了对 Master 状态简单可靠的更新，在 Master 宕机时不用担心一致性问题。但是，Master 不会永久存储块位置信息。在 Master 启动以及有新的块服务器加入集群时，向每个块服务器拉去块信息。这种方法允许系统动态调整群集中的变化，例如添加或删除区块服务器。总体而言，Master 在管理 GFS 元数据方面起着至关重要的作用，其设计是性能、可靠性和可扩展性之间的平衡。



### 2.6.1、内存数据结构

​	GFS 将元数据存储在内存中，Master 可以快速的执行命令。由于元数据都在内存中，Master 可以在快速高效地定期扫描全部状态。这种周期任务用于各种活动，例如区块垃圾收集、块服务器故障时的重新复制以及块迁移以平衡块服务器之间的负载和磁盘空间使用情况。4.3、4.4节会详细介绍这些内容。

​	纯内存方式的一个潜在问题是，块的数量以及整个系统的容量都受到 Master 内存容量的限制。但是，这在实践中并不是一个严重的限制，因为 Master 为每个 64 MB 区块保留的元数据少于 64 字节。由于顺序填充，文件的大多数块是填满的，只有最后一个可能被部分填满，所以单个大文件对应的块数量不多。通常每个文件命名空间所需的字节少于 64 字节，因为它使用前缀压缩来紧凑地存储文件名。

​	如果需要支持更大容量的文件系统，向 Master 添加额外内存的成本很小，而将元数据存储在内存中可获得的简单性、可靠性、性能和灵活性。



### 2.6.2、块地址

>   Master 控制着块信息的变更，无需块服务器定时上报块变更？

​	Master 不会持久化块服务器上的块信息。相反，它会在启动时轮询 chunkservers 以获取这些信息。 此后，主节点可以保持最新状态，因为它可以控制所有区块的位置，并通过常规的 HeartBeat 消息监视块服务器状态。 

​	最初，作者试图将块位置信息永久保存在 Master，但在启动时以及之后定期向块服务器请求数据要简单得多。 此设计消除了在 chunkservers 加入和离开集群、更改名称、失败、重启等时保持 Master 和块服务器同步的问题。在拥有数百台服务器的集群中，这些事件经常发生。

​	此外，块服务器在自己的磁盘上有哪些区块或没有哪些区块的最终决定权在于 chunkserver 本身。因此，在 Master 上保持对这些信息的一致视图是没有意义的，因为块服务器上的错误可能会导致块自发消失（例如，磁盘可能变坏并被禁用），或者人为重命名 chunkserver。如果 Master 要维持每个块服务器上块信息的一致视图，则需要不断检查和更新这些信息，这会效率低下而且可能不可靠。通过允许每个 chunkserver 管理自己的区块，系统可以提高容错能力和对错误的弹性。



### 2.6.3、操作日志

​	操作日志是 GFS 的重要组成部分，其中包含关键元数据变更的历史记录。它是元数据的唯一持久化记录，也是定义并发操作顺序的逻辑时间表。创建文件、区块及其版本的逻辑时间只由操作日志识别。

​	由于操作日志至关重要，因此必须对其进行可靠存储，并在变更持久化前，不应让客户端感知变更。如果在变更持久化前已对客户端可见，则即使块本身仍然存在，整个文件系统或最近的客户端操作也可能会丢失。为确保可靠性，将在多节点上复制操作日志，只有在本地和远程都将日志落盘后，才会响应客户端操作。Master 在刷新前将多个日志打包批处理，从而减少刷新和复制对整体系统吞吐量的影响。

​	Master 通过重播操作日志来恢复其文件系统状态。 为了最大限度地缩短启动时间，日志必须保持较小。 每当日志超过一定大小时，Master 都会检查其状态。 检查点以类似 B 树的紧凑形式创建，可以直接映射到内存中，无需额外解析即可用于命名空间查找。 这进一步加快了恢复速度并提高了可用性。

​	由于构建检查点可能需要一段时间，Master 内部状态的设计成可以在创建新的检查点时不阻塞变更。 Master 切换到新的日志文件并在单独的线程中创建新的检查点（copy 某一瞬间的快照？）。新的检查点包括切换前的所有变更，对于包含几百万个文件的集群，可以在一分钟左右的时间内创建。 完成后，新的检查点将在本地和远程写入磁盘。

​	恢复只需要最新的完整检查点和后续的日志文件。 可以自由删除较旧的检查点和日志文件，但为了防范灾难，保留了一些检查点和日志文件。 检查点操作期间出现故障不会影响正确性，因为恢复代码会检测并跳过不完整的检查点。

